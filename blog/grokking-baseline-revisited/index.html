<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},i=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),a=i[0][0],r=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"July 21, 2025"),o="Grokking Baseline Revisited",l="The ICLR 2025 paper, Grokking at the Edge of Numerical Stability, (Prieto et al., 2025) and the invention of Muon optimizer have renewed interest in classic grokking experiments and therefore raised the importance of accessing how well the baseline can perform in such experiments. Here we report replication failures of some of the experiments in Prieto et al., 2025 and improved AdamW MLP baseline on modular addition. With tuned learning rate and weight decay AdamW performs surprisingly well and may be hard to improve upon.";{let e=i.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(a+"2025"+o.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${o}},\n  abstract = {${l}},\n  booktitle = {EIFY's site},\n  year = {2025},\n  date = {${r}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=i.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${o}", EIFY's site, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Grokking Baseline Revisited | EIFY's site</title> <meta name="author" content=" "> <meta name="description" content="The ICLR 2025 paper, Grokking at the Edge of Numerical Stability, (Prieto et al., 2025) and the invention of Muon optimizer have renewed interest in classic grokking experiments and therefore raised the importance of accessing how well the baseline can perform in such experiments. Here we report replication failures of some of the experiments in Prieto et al., 2025 and improved AdamW MLP baseline on modular addition. With tuned learning rate and weight decay AdamW performs surprisingly well and may be hard to improve upon."> <meta name="keywords" content="machine-learning, ml, deep-learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://eify.github.io//blog/grokking-baseline-revisited/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">r{color:Red}g{color:Green}gr{color:Gray}</style> <d-front-matter> <script async type="text/json">{
      "title": "Grokking Baseline Revisited",
      "description": "The ICLR 2025 paper, Grokking at the Edge of Numerical Stability, (Prieto et al., 2025) and the invention of Muon optimizer have renewed interest in classic grokking experiments and therefore raised the importance of accessing how well the baseline can perform in such experiments. Here we report replication failures of some of the experiments in Prieto et al., 2025 and improved AdamW MLP baseline on modular addition. With tuned learning rate and weight decay AdamW performs surprisingly well and may be hard to improve upon.",
      "published": "July 21, 2025",
      "authors": [
        {
          "author": "Jason Chuan-Chih Chou",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Cohere Labs Community",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">EIFY's site</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Grokking Baseline Revisited</h1> <p>The ICLR 2025 paper, Grokking at the Edge of Numerical Stability, (Prieto et al., 2025) and the invention of Muon optimizer have renewed interest in classic grokking experiments and therefore raised the importance of accessing how well the baseline can perform in such experiments. Here we report replication failures of some of the experiments in Prieto et al., 2025 and improved AdamW MLP baseline on modular addition. With tuned learning rate and weight decay AdamW performs surprisingly well and may be hard to improve upon.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#discrepancies-of-prieto-et-al-2025">Discrepancies of Prieto et al., 2025</a></div> <div><a href="#adamw-wd-baseline">AdamW WD baseline</a></div> <div><a href="#lipschitz-measurements">Lipschitz measurements</a></div> <div><a href="#conclusions">Conclusions</a></div> <div><a href="#replication-guide">Replication guide</a></div> <ul> <li><a href="#adamw-wd-baseline">AdamW WD baseline</a></li> <li><a href="#lipschitz-measurements">Lipschitz measurements</a></li> </ul> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>First reported in <d-cite key="power2022grokkinggeneralizationoverfittingsmall"></d-cite>, “grokking” refers to the phenomenon of extremely delayed generalization during model training, in which the model reaches near-perfect accuracy on the training set orders of magnitude earlier than on the test set. More recently, the ICLR 2025 paper “Grokking at the Edge of Numerical Stability” (Prieto et al., 2025, <d-cite key="prieto2025grokking"></d-cite>) reports improved grokking performance with modified softmax function and AdamW optimizer <d-cite key="loshchilov2018decoupled"></d-cite>, and people have been benchmarking variants of Muon optimizer <d-cite key="jordan2024muon"></d-cite> against the AdamW baseline on grokking experiments <d-cite key="tveit2025muonoptimizeracceleratesgrokking"></d-cite><d-cite key="cesista2025spectralclipping"></d-cite><d-cite key="EssentialAI2025muongrokking"></d-cite>. It is perhaps imperative, therefore, to double-check how well the AdamW baseline can really perform for these experiments. Following <d-cite key="prieto2025grokking"></d-cite>, all the experiments presented in this post are on the AdamW baseline from the paper with a 2-hidden layer multi-layer perceptron (MLP) of width 200 for modular addition: \((a + b)\, \mathrm{mod} \, p\) where \(a, b \in [0, p), \, p = 113\) unless specified otherwise.</p> <h2 id="discrepancies-of-prieto-et-al-2025">Discrepancies of Prieto et al., 2025</h2> <p>We start by replicating the results of Prieto et al., 2025 based on <a href="https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability" rel="external nofollow noopener noopener noreferrer" target="_blank">the official repo</a> but encounter unexpected issues, some of which are likely inadvertent:</p> <ol> <li> <p><a href="https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability/issues/6" rel="external nofollow noopener noopener noreferrer" target="_blank">Discrepancy between <code class="language-plaintext highlighter-rouge">AlgorithmicDataset</code> and <code class="language-plaintext highlighter-rouge">AlgorithmicDatasetTransformer</code></a></p> <p>While intended to generate equivalent datasets with different input format (1-hot embedding vs. class index), these two dataset classes are in fact not equivalent. The input of the former are in the range \([0, p)\):</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">):</span>
         <span class="c1"># (...)
</span></code></pre></div> </div> <p>While the input of the latter are in the range \([1, p)\):</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="n">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="p">)],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
</code></pre></div> </div> <p>This means that the results of the transformer models are not fully comparable to that of the MLP models. For \(p = 113\) the difference is &lt;2% but since zero is the additive identity, the impact on the model training and evaluation can be disproportionate.</p> </li> <li> <p><a href="https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability/issues/7" rel="external nofollow noopener noopener noreferrer" target="_blank">Fig. 1a experiment</a></p> <p>Legend of Fig. 1 indicates that the experiment for Fig. 1a is run with modified AdamW optimizer (\(\perp\)AdamW) and modified softmax function (StableMax):</p> <div class="caption"> <img src="/assets/img/2025-07-21-grokking-baseline-revisited/figure_1_crop.png" class="img-fluid" width="auto" height="auto"> </div> <p>The correponding command in the experiment script, however, defaults to the standard softmax function:</p> <div class="language-bash highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> python grokking_experiments.py <span class="nt">--lr</span> 0.01 <span class="nt">--num_epochs</span> 300 <span class="nt">--log_frequency</span> 10 <span class="nt">--device</span> <span class="s2">"</span><span class="nv">$DEVICE</span><span class="s2">"</span> <span class="nt">--train_fraction</span> 0.4 <span class="nt">--beta2</span> 0.99 <span class="nt">--orthogonal_gradients</span>
</code></pre></div> </div> <p>The model underperforms when the same command is run with <code class="language-plaintext highlighter-rouge">--loss_function stablemax</code>:</p> <div class="caption"> <img src="/assets/img/2025-07-21-grokking-baseline-revisited/pAdamW_stablemax.png" class="img-fluid" width="50%" height="auto"> </div> </li> <li> <p><a href="https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability/issues/11" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">betas</code> and <code class="language-plaintext highlighter-rouge">eps</code> are never set for \(\perp\)AdamW experiments</a></p> <p>\(\perp\)AdamW runs AdamW as the <code class="language-plaintext highlighter-rouge">base_optimizer</code> after orthogonalizing the gradients, but only LR and WD (weight decay) coefficient are passed for its initialization. Consequently, the rest of the hyperparameters always default to the PyTorch default despite obvious intention to change \(\beta_2\) and \(\epsilon\).</p> </li> </ol> <p>Others, however, are harder to explain:</p> <ol> <li> <p><a href="https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability/issues/8" rel="external nofollow noopener noopener noreferrer" target="_blank">Weight-decay (WD) experiments of Fig. 15a</a></p> <p>See <a href="#adamw-wd-baseline">next section</a>.</p> </li> <li> <a href="https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability/issues/9" rel="external nofollow noopener noopener noreferrer" target="_blank">Fig. 2a reproduction issues</a> <div class="caption"> <img src="/assets/img/2025-07-21-grokking-baseline-revisited/figure_2a_crop.png" class="img-fluid" width="50%" height="auto"> </div> <p>Fig. 2a above purportedly shows that models trained on 0.4 of the dataset all exhibit “softmax collapse” (SC) and reach 50% SC before making any progress on test accuracy. The provided plotting code, however, shows that the vertical dotted lines supposedly mark the time when 40% SC is reached:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="n">ax</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">softmax_collapse_16</span><span class="o">&gt;</span><span class="mf">0.4</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">log_frequency</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">50% zero terms in the loss</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">ax</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">softmax_collapse_32</span><span class="o">&gt;</span><span class="mf">0.4</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">log_frequency</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">50% zero terms in the loss</span><span class="sh">"</span><span class="p">)</span>
 <span class="n">ax</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">softmax_collapse_64</span><span class="o">&gt;</span><span class="mf">0.4</span><span class="p">))[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">log_frequency</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">50% zero terms in the loss</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> <p>Furthermore, inspection of the logged metrics shows that the softmax precision 32 and 64 experiments never reach 40% and 50% SC, respectively.</p> </li> <li> <p><a href="https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability/issues/10" rel="external nofollow noopener noopener noreferrer" target="_blank">Fig. 6 reproduction failure</a></p> <div class="caption"> <img src="/assets/img/2025-07-21-grokking-baseline-revisited/figure_6_crop.png" class="img-fluid" width="auto" height="auto"> </div> <p>Fig. 6b above fails to reproduce with the published code, script, and plotting notebook. In particular, \(\perp\)AdamW significantly underperforms the shown result:</p> <div class="caption"> <img src="/assets/img/2025-07-21-grokking-baseline-revisited/figure_6b_repro.png" class="img-fluid" width="50%" height="auto"> </div> <p>Perhaps less critically, the transformer experiments of Fig. 6a are in fact performing modular addition instead of subtraction according to the experiment script. These experiments are also affected by the <a href="https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability/issues/6" rel="external nofollow noopener noopener noreferrer" target="_blank">dataset discrepancy</a> although the qualitative result may not change.</p> </li> </ol> <h2 id="adamw-wd-baseline">AdamW WD baseline</h2> <p>For the field as a whole, however, perhaps the most important question is how well the AdamW WD baseline can perform for these grokking experiments. In Fig. 15 of Prieto et al., 2025, the authors sweep the WD coefficient and compare the experiments with the best setting with that of \(\perp\)AdamW:</p> <div class="caption"> <img src="/assets/img/2025-07-21-grokking-baseline-revisited/figure_15_crop.png" class="img-fluid" width="auto" height="auto"> </div> <p>While Prieto et al. provide no details for these experiments, WD experiments run with the most common setup of the main experiments (LR=0.01 and trained with 0.4 of the dataset) significantly outperform the ones shown in Fig. 15a:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/env bash</span>
<span class="nv">DEVICE</span><span class="o">=</span><span class="s2">"cuda:0"</span>
<span class="k">for </span>WD <span class="k">in </span>2. 4. 6. 8. 10.
<span class="k">do
    </span>python grokking_experiments.py <span class="nt">--lr</span> 0.01 <span class="nt">--weight_decay</span> <span class="nv">$WD</span> <span class="nt">--num_epochs</span> 1001 <span class="nt">--log_frequency</span> 10 <span class="nt">--device</span> <span class="nv">$DEVICE</span> <span class="nt">--train_fraction</span> 0.4
<span class="k">done</span>
</code></pre></div></div> <div class="caption"> <img src="/assets/img/2025-07-21-grokking-baseline-revisited/figure_15a_repro_zoom_in.png" class="img-fluid" width="50%" height="auto"> </div> <p>Generalization of the WD experiments can be further sped up with LR tuning so that \(\perp\)AdamW no longer appears favorable:</p> <div class="caption"> <img src="/assets/img/2025-07-21-grokking-baseline-revisited/best_experiments.png" class="img-fluid" width="50%" height="auto"> </div> <p>For a more systematic comparison, we test architecture variations along two axes<d-footnote>Prieto et al.’s MLP 2-hot encodes the operands of modular addition $$a, b \in [0, p)$$ by concatenating their 1-hot encoding. Prieto et al. claim that this MLP is from <d-cite key="liu2023omnigrok"></d-cite>, which in turn claims that the setup is from Liu et al., 2022 <d-cite key="liu2022towards"></d-cite>. Liu et al.'s repository, however, shows that <a href="https://github.com/ejmichaud/grokking-squared/blob/0229df94de69b8384e560367280a43a238112bf5/toy/train_add.py#L31-L33" rel="external nofollow noopener noopener noreferrer" target="_blank">its toy modular addition model first adds up the embeddings of a, b before feeding the sum to the 2-hidden layer MLP</a>. In contrast, Prieto et al.’s MLP feeds the 2-hot encoding directly to the MLP so its first hidden layer effectively adds up the "embeddings" of a, b and the overall model has one less hidden layer, in addition of the difference of nonlinearity (tanh vs. ReLU) and the fact that Prieto et al.'s MLP applies ReLU to the sum of "embeddings" first. The architecture variations we test are motivated by this discrepancy and the dimension of the trainable embeddings is set to 100 to keep the dimension of the concatenated embedding the same as that of the hidden layers (200).</d-footnote>:</p> <ol> <li>Number of hidden layers \(\in \{1, 2, 3\}\)</li> <li>Trainable embedding layer instead of 2-hot encoding for the operands \(a, b\)</li> </ol> <p>For each variation we grid-search LR \(\in \{0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1\}\) for both the \(\perp\)AdamW and WD experiments. For the WD experiments, we also grid-search the WD coefficient \(\in \{2, 4, 6, 8, 10\}\). We then compare the first epoch to solve modular addition with \(p = 113\) (the first epoch that the model reaches 100% accuracy on the test set) between the best WD experiment and the best \(\perp\)AdamW experiment for each architecture variation. The difference is marked <g>green</g> if the best WD experiment solves first and <r>red</r> if the best \(\perp\)AdamW experiment solves first. We do not train beyond 400 epochs for the 3 hidden layers experiments since smaller models can solve modular addition within half of the training budget:</p> <table> <thead> <tr> <th style="text-align: center"> </th> <th style="text-align: center">2-hot Encoding</th> <th style="text-align: center">Embedding Layer</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1 Hidden Layer</td> <td style="text-align: center">917 <g>-119</g> </td> <td style="text-align: center">126 <g>-140</g> </td> </tr> <tr> <td style="text-align: center">2 Hidden Layers</td> <td style="text-align: center">143 <r>+19</r> </td> <td style="text-align: center">111 <g>-87</g> </td> </tr> <tr> <td style="text-align: center">3 Hidden Layers</td> <td style="text-align: center">&gt;400 <gr>±?</gr> </td> <td style="text-align: center">146 <g>-&gt;254</g> </td> </tr> </tbody> </table> <p>We can see that \(\perp\)AdamW doesn’t hold a systematic advantage over the simple WD baseline. In fact, it underperforms across the board with more conventional trainable embedding. We have also tested setting WD to 0 for the embedding layer and running the PyTorch default \(\beta_2\) and \(\epsilon\) for \(\perp\)AdamW like the original repo, but neither helps.</p> <h2 id="lipschitz-measurements">Lipschitz measurements</h2> <p>Cesista, 2025 <d-cite key="cesista2025spectralclipping"></d-cite> has experimented on a variant of the MLP model and measured how fast variants of the Muon optimizer can train the model to grok vs. how robust the models are. How does the AdamW WD baseline compare?</p> <p>To recap, one way to measure the model’s robustness is to measure the Lipschitz constant $L$:</p> <blockquote> <p><strong>Definition (Lipschitz)</strong>. Let $f: \mathbb{R}^n \to \mathbb{R}^m$ be a function, then $f$ is said to be $L$-Lipschitz continuous if there exists a constant $L \geq 0$ such that for all $x, y \in \mathbb{R}^n$, \(||f(x) - f(y)|| \leq L||x - y||\) for some norm $||\cdot||$ chosen a priori.</p> </blockquote> <p>Smaller Lipschitz constant $L$ means that the model is less sensitive to input perturbation, therefore can be considered more robust. In our case of a 2-hidden layer MLP that takes the concatenation of two 113-dim embeddings as the input and outputs the 113-dim logit, $n = 226$ and $m = 113$. With L2 norm $||\cdot||_2$ as the norm of choice and 1-Lipschitz functions such as ReLU or GELU as the activation function, the product of the spectral norms of the 3 linear layers constitutes a upper bound of the Lipschitz constant $L$ where the spectral norm of a linear transformation $M$ is its largest singular value:</p> \[||M||_2 = \max_{x \neq 0} \frac{||Mx||_2}{||x||_2}\] <p>which can efficiently approximated with <a href="https://en.wikipedia.org/wiki/Power_iteration" rel="external nofollow noopener noopener noreferrer" target="_blank">power iteration</a>. Following Cesista, 2025 <d-cite key="cesista2025spectralclipping"></d-cite>, we train such MLP with GELU activation function, AdamW, and bfloat16 precision to grok modular addition and modular multiplication with $p=113$. We again grid-search LR \(\in \{0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1\}\) and the WD coefficient \(\in \{2, 4, 6, 8, 10\}\) and run 64 random seeds for each setting. We filter out settings in which the model doesn’t reach 95% test accuracy 100% of the time within 1000 epochs (equivalent to 1000 steps with full-batch training), and report the median epochs to grok and the median Lipschitz upper bound of the remaining settings:</p> <div class="caption"> <img src="/assets/img/2025-07-21-grokking-baseline-revisited/gelu_epoch_lipschitz.png" class="img-fluid" width="50%" height="auto"> </div> <p>Compared to the results reported in Cesista, 2025 <d-cite key="cesista2025spectralclipping"></d-cite>, AdamW breaks the Pareto frontier of Muon variants with models trained to grok within 200 epochs and Lipschitz upper bound &lt; 200. We don’t see clear correlation between epochs to grok and the model’s Lipschitz upper bound. Switching from GELU to ReLU doesn’t change the result significantly.</p> <h2 id="conclusions">Conclusions</h2> <p>Inline with <d-cite key="EssentialAI2025muongrokking"></d-cite>, we find that AdamW with tuned LR and WD coefficient to be a strong baseline for the task of modular addition and multiplication in comparison to modified \(\perp\)AdamW and variants of Muon. In fact, with the tasks solved within 150 epochs, perhaps we should question whether such training dynamics should still be called “grokking” and whether other common grokking tasks can be similarly solved efficiently with tuned LR and WD. To the extent that it is still relevant, the AdamW baseline for grokking may be hard to improve upon significantly.</p> <h2 id="replication-guide">Replication guide</h2> <h3 id="adamw-wd-baseline-1">AdamW WD baseline</h3> <ol> <li><code class="language-plaintext highlighter-rouge">git clone -b talon https://github.com/EIFY/grokking-at-the-edge-of-numerical-stability.git</code></li> <li><code class="language-plaintext highlighter-rouge">cd grokking-at-the-edge-of-numerical-stability</code></li> <li><code class="language-plaintext highlighter-rouge">./wd_experiments.sh</code></li> <li>Run jupyter notebook <code class="language-plaintext highlighter-rouge">wd_plots.ipynb</code> </li> </ol> <h3 id="lipschitz-measurements-1">Lipschitz measurements</h3> <ol> <li><code class="language-plaintext highlighter-rouge">git clone -b lipschitz https://github.com/EIFY/grokking-at-the-edge-of-numerical-stability.git</code></li> <li><code class="language-plaintext highlighter-rouge">cd grokking-at-the-edge-of-numerical-stability</code></li> <li><code class="language-plaintext highlighter-rouge">./lipschitz_experiments.sh # This takes hours due to running 64 random seeds for each setting</code></li> <li>Run jupyter notebook <code class="language-plaintext highlighter-rouge">lipschitz_plots.ipynb</code> </li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/assets/bibliography/2025-07-21-grokking-baseline-revisited.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>