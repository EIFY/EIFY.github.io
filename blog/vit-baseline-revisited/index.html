<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),i=a[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"July 3, 2025"),r="ViT Baseline Revisited",m="ViT (vision transformer) has emerged as a major model family for computer vision with the same architecture as that of dominant LLMs and performance matching or exceeding that of the CNN-based ResNet-like models. Shortly after the ICLR publication, a note was published to follow up with better performance of the smaller ViT-S/16 variant on the ImageNet-1k dataset. In our effort to reproduce that, we find inconsistencies among major implementations of ViT, RandAugment, and Inception crop that impact model performance. We achieve better performance with 90 / 150 epoch training budget and call for better awareness of implementation discrepancies.";{let e=a.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${m}},\n  booktitle = {EIFY's site},\n  year = {2025},\n  date = {${o}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=a.map(e=>e[0]),t=`\n${e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0]}, "${r}", EIFY's site, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>ViT Baseline Revisited | EIFY's site</title> <meta name="author" content=" "> <meta name="description" content="ViT (vision transformer) has emerged as a major model family for computer vision with the same architecture as that of dominant LLMs and performance matching or exceeding that of the CNN-based ResNet-like models. Shortly after the ICLR publication, a note was published to follow up with better performance of the smaller ViT-S/16 variant on the ImageNet-1k dataset. In our effort to reproduce that, we find inconsistencies among major implementations of ViT, RandAugment, and Inception crop that impact model performance. We achieve better performance with 90 / 150 epoch training budget and call for better awareness of implementation discrepancies."> <meta name="keywords" content="machine-learning, ml, deep-learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://eify.github.io//blog/vit-baseline-revisited/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">r{color:Red}g{color:Green}</style> <d-front-matter> <script async type="text/json">{
      "title": "ViT Baseline Revisited",
      "description": "ViT (vision transformer) has emerged as a major model family for computer vision with the same architecture as that of dominant LLMs and performance matching or exceeding that of the CNN-based ResNet-like models. Shortly after the ICLR publication, a note was published to follow up with better performance of the smaller ViT-S/16 variant on the ImageNet-1k dataset. In our effort to reproduce that, we find inconsistencies among major implementations of ViT, RandAugment, and Inception crop that impact model performance. We achieve better performance with 90 / 150 epoch training budget and call for better awareness of implementation discrepancies.",
      "published": "July 3, 2025",
      "authors": [
        {
          "author": "Jason Chuan-Chih Chou",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Cohere Labs Community",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">EIFY's site</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>ViT Baseline Revisited</h1> <p>ViT (vision transformer) has emerged as a major model family for computer vision with the same architecture as that of dominant LLMs and performance matching or exceeding that of the CNN-based ResNet-like models. Shortly after the ICLR publication, a note was published to follow up with better performance of the smaller ViT-S/16 variant on the ImageNet-1k dataset. In our effort to reproduce that, we find inconsistencies among major implementations of ViT, RandAugment, and Inception crop that impact model performance. We achieve better performance with 90 / 150 epoch training budget and call for better awareness of implementation discrepancies.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#implementation-discrepancies">Implementation discrepancies</a></div> <ul> <li><a href="#vit-initialization">ViT initialization</a></li> <li><a href="#randaugment">RandAugment</a></li> <li><a href="#inception-crop">Inception crop</a></li> <li><a href="#big-vision-miscellaneous">Big Vision miscellaneous</a></li> </ul> <div><a href="#corrected-reproduction">Corrected reproduction</a></div> <div><a href="#schedule-free-baseline">Schedule-Free baseline</a></div> <div><a href="#implication">Implication</a></div> <div><a href="#conclusion">Conclusion</a></div> <ul> <li><a href="#replication-guide">Replication guide</a></li> </ul> </nav> </d-contents> <p><em>Edited and expanded from <a href="https://github.com/EIFY/mup-vit" rel="external nofollow noopener noopener noreferrer" target="_blank">EIFY/mup-vit/README.md</a>.</em></p> <h2 id="introduction">Introduction</h2> <p>The dawn of the deep learning era is marked by the “Imagenet moment”, when Alexnet won the ILSVRC-2012 competition for classifying the ImageNet-1k dataset <d-cite key="10.1145/3065386"></d-cite>. Since then, the field of CV (computer vision) had been dominated by deep CNNs (convolutional neural networks), especially ResNet-like ones featuring residual connections <d-cite key="7780459"></d-cite>. In 2020, however, Dosovitskiy et al. proposed vision transformer (ViT) <d-cite key="dosovitskiy2020image"></d-cite>, which uses transformer architecture that is already dominating the field of language modeling for CV. With competitive performance, ViT is now widely adapted not only for CV tasks, but also as a component for vision-language models <d-cite key="radford2021learning"></d-cite>.</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/model_scheme.png" class="img-fluid" width="auto" height="auto"> </div> <div class="caption"> ViT architecture. Images are divided into square patches, which are then linearly projected into tokens. After adding position embedding, the tokens are fed to the pre-norm transformer encoder. In the original architecture, an extra classification token is added to the input sequence and its output is used for classification ("token pooling"). Figure from <d-cite key="dosovitskiy2020image"></d-cite>. </div> <p>Since transformer encoder is permutation-invariant, ViT is considered to have weaker inductive bias and relies more on model regularization or data augmentation, especially at smaller scale <d-cite key="steiner2022how"></d-cite>. It is therefore particularly notable that Beyer et al. published a short note claiming that ViT-S/16 can achieve better metrics on the ImageNet-1k dataset than ResNet-50 <d-cite key="beyer2022better"></d-cite>. With manageable compute requirement and open-sourced Big Vision repo <d-cite key="big_vision"></d-cite> as the reference implementation, we decide to reproduce it in the PyTorch <d-cite key="Ansel_PyTorch_2_Faster_2024"></d-cite> ecosystem as the entry point to ViT research.</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">Layers</th> <th style="text-align: center">Width</th> <th style="text-align: center">MLP</th> <th style="text-align: center">Heads</th> <th style="text-align: center">Params</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">ViT-Ti<d-cite key="pmlr-v139-touvron21a"></d-cite> </td> <td style="text-align: center">12</td> <td style="text-align: center">192</td> <td style="text-align: center">768</td> <td style="text-align: center">3</td> <td style="text-align: center">5.8M</td> </tr> <tr> <td style="text-align: center">ViT-S<d-cite key="pmlr-v139-touvron21a"></d-cite> </td> <td style="text-align: center">12</td> <td style="text-align: center">384</td> <td style="text-align: center">1536</td> <td style="text-align: center">6</td> <td style="text-align: center">22.2M</td> </tr> <tr> <td style="text-align: center">ViT-B<d-cite key="dosovitskiy2020image"></d-cite> </td> <td style="text-align: center">12</td> <td style="text-align: center">768</td> <td style="text-align: center">3072</td> <td style="text-align: center">12</td> <td style="text-align: center">86M</td> </tr> <tr> <td style="text-align: center">ViT-L<d-cite key="dosovitskiy2020image"></d-cite> </td> <td style="text-align: center">24</td> <td style="text-align: center">1024</td> <td style="text-align: center">4096</td> <td style="text-align: center">16</td> <td style="text-align: center">307M</td> </tr> </tbody> </table> <div class="caption"> A few common ViT sizes. <d-cite key="beyer2022better"></d-cite> and this blogpost exclusively focuses on ViT-S/16, a variant of ViT-S with patch size $16 \times 16$. Table from <d-cite key="steiner2022how"></d-cite>. </div> <h2 id="implementation-discrepancies">Implementation discrepancies</h2> <p>The variant of ViT-S/16 used in <d-cite key="beyer2022better"></d-cite> differs from the original ViT:</p> <ol> <li>Instead of token pooling, average of all the output tokens (“global average-pooling”, GAP) <d-cite key="9711302"></d-cite> is fed to the MLP head for classification.</li> <li>Fixed 2D sin-cos position embeddings <d-cite key="9711302"></d-cite> is used instead of learned position embeddings.</li> </ol> <p>The model is then trained with Inception crop <d-cite key="7298594"></d-cite>, random horizontal flips, RandAugment <d-cite key="NEURIPS2020_d85b63ef"></d-cite>, and Mixup <d-cite key="zhang2018mixup"></d-cite>. Furthermore, there is a variant that replaces the MLP head with a single linear layer that we initially focus on since it is even simpler and makes “no significant difference” <d-cite key="beyer2022better"></d-cite>. Sometimes called “simple ViT” <d-cite key="vit-pytorch"></d-cite>, it turns out that there is no up-to-date implementation in PyTorch readily available<d-footnote>simple_vit.py from vit-pytorch does not support modern attention kernels e.g. FlashAttention <d-cite key="dao2022flashattention"></d-cite>. The simple_flash_attn_vit.py variant does but is heavily defensive and resorts to the functional F.scaled_dot_product_attention() for backward compatibility. </d-footnote>, so we decide to implement our own.</p> <h3 id="vit-initialization">ViT initialization</h3> <p>It turns out that even building simple ViT from built-in modules from PyTorch requires reinitializing most of the parameters to match that of the Big Vision reference implementation, including:</p> <ol> <li> <p><code class="language-plaintext highlighter-rouge">torch.nn.MultiheadAttention</code></p> <ul> <li> <p><code class="language-plaintext highlighter-rouge">in_proj_weight</code>: In the most common use case when the value dimension is equal to query and key dimension, their projection matrices are combined into <code class="language-plaintext highlighter-rouge">in_proj_weight</code> whose initial values are <a href="https://github.com/pytorch/pytorch/blob/aafb3deaf1460764432472a749d625f03570a53d/torch/nn/modules/activation.py#L1112" rel="external nofollow noopener noopener noreferrer" target="_blank">set with <code class="language-plaintext highlighter-rouge">xavier_uniform_()</code></a>. Likely unintentionally, this means that the values are sampled from uniform distribution $\mathcal{U}(-a, a)$ where \(a = \sqrt{\frac{3}{2 \text{ hidden_dim}}}\) instead of \(\sqrt{\frac{3}{\text{hidden_dim}}}\)<d-footnote>This is not the only case where the combined QKV projection matrix misleads shape-aware optimizer and parameter initialization (<a href="https://x.com/kellerjordan0/status/1844820920780947800" rel="external nofollow noopener noopener noreferrer" target="_blank">1</a>, <a href="https://github.com/graphcore-research/unit-scaling/blob/1edf20543439e042cc314667b348d9b4c4480e23/unit_scaling/_modules.py#L479" rel="external nofollow noopener noopener noreferrer" target="_blank">2</a>). It may be worth reexamining whether this truly leads to better performance and whether it is worth it.</d-footnote>.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">out_proj.weight</code>: Furthermore, the output projection is <a href="https://github.com/pytorch/pytorch/blob/e62073d7997c9e63896cb5289ffd0874a8cc1838/torch/nn/modules/activation.py#L1097" rel="external nofollow noopener noopener noreferrer" target="_blank">initialized as <code class="language-plaintext highlighter-rouge">NonDynamicallyQuantizableLinear</code></a>. Just like <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.Linear</code></a>, its initial weights are sampled from uniform distribution $\mathcal{U}(-a, a)$ where \(a = \sqrt{\frac{1}{\text{hidden_dim}}}\) instead of \(\sqrt{\frac{3}{\text{hidden_dim}}}\) <d-footnote>See <a href="https://github.com/pytorch/pytorch/issues/57109#issuecomment-828847575" rel="external nofollow noopener noopener noreferrer" target="_blank">pytorch/pytorch#57109 (comment)</a> for the origin of this discrepancy.</d-footnote>.</p> </li> </ul> <p>To conform with <a href="https://github.com/google-research/big_vision/blob/ec86e4da0f4e9e02574acdead5bd27e282013ff1/big_vision/models/vit.py#L93" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">jax.nn.initializers.xavier_uniform()</code> used by the reference implementation</a>, both are re-initialized with samples from uniform distribution $\mathcal{U}(-a, a)$ where \(a = \sqrt{\frac{3}{\text{hidden_dim}}}\) <d-footnote>Note that the standard deviation of uniform distribution $\mathcal{U}(-a, a)$ is $$\frac{a}{\sqrt{3}}$$ So this is also the correct initialization for preserving the input scale assuming that the input features are uncorrelated.</d-footnote>.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">torch.nn.Conv2d</code>: Linear projection of flattened patches can be done with 2D convolution, namely <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.Conv2d</code> initialized with <code class="language-plaintext highlighter-rouge">in_channels = 3</code>, <code class="language-plaintext highlighter-rouge">out_channels = hidden_dim</code>, and both <code class="language-plaintext highlighter-rouge">kernel_size</code> and <code class="language-plaintext highlighter-rouge">stride</code> set to <code class="language-plaintext highlighter-rouge">patch_size</code> in PyTorch</a>. <code class="language-plaintext highlighter-rouge">torch.nn.Conv2d</code>, however, defaults to weight and bias initialization with uniform distribution $\mathcal{U}(-a, a)$ where \(a = \sqrt{\frac{1}{\text{fan_in}}}\) instead of Big Vision’s Lecun normal (truncated normal) initialization <d-cite key="klambauer2017self"></d-cite> for weight and zero-init for bias. Furthermore, <a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.trunc_normal_" rel="external nofollow noopener noopener noreferrer" target="_blank">PyTorch’s own <code class="language-plaintext highlighter-rouge">nn.init.trunc_normal_()</code></a> doesn’t take the effect of truncation on standard deviation into account unlike <a href="https://github.com/google/jax/blob/1949691daabe815f4b098253609dc4912b3d61d8/jax/_src/nn/initializers.py#L334" rel="external nofollow noopener noopener noreferrer" target="_blank">the JAX implementation</a>, so we have to implement the correction ourselves:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/truncated_normal.png" class="img-fluid" width="auto" height="auto"> </div> <div class="caption"> Unit normal distribution has standard deviation $\sigma = 1$, but after truncating at $\pm 2$ the standard deviation is reduced to $\sigma = 0.880$. To restore unit standard deviation, one needs to sample $\mathcal{N}(0, \sigma = \frac{1}{.880})$ instead and truncate at $\pm 2 \sigma$. </div> </li> <li> <p><code class="language-plaintext highlighter-rouge">torch.nn.Linear</code> for the classification head: Specifically for the classification head, Big Vision usually zero-inits both the weight and bias for the linear layer, including the ViT-S/16 in question. Notably, neither <a href="https://github.com/lucidrains/vit-pytorch/blob/141239ca86afc6e1fe6f4e50b60d173e21ca38ec/vit_pytorch/simple_vit.py#L108" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">simple_vit.py</code></a> nor <a href="https://github.com/lucidrains/vit-pytorch/blob/141239ca86afc6e1fe6f4e50b60d173e21ca38ec/vit_pytorch/simple_flash_attn_vit.py#L162" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">simple_flash_attn_vit.py</code></a> from vit-pytorch does this.</p> </li> </ol> <p>After fixing 1-3, we verify that all of the per-layer summary statistics including minimum, maximum, mean, and standard deviation of the 21,974,632 model parameters at initialization match that of the Big Vision reference implementation.</p> <h3 id="randaugment">RandAugment</h3> <p>Following <d-cite key="beyer2022better"></d-cite>, we train the model with batch size 1024, training budget 90 epochs = round(1281167 / 1024 * 90) = 112603 steps, 10000 warm-up steps, cosine learning rate decay, 1e-3 maximum learning rate, 1e-4 decoupled weight decay <d-cite key="adamw-decoupling-blog"></d-cite> (0.1 in PyTorch’s parameterization before multiplying by the learning rate), AdamW optimizer, gradient clip with <a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">max_norm = 1.0</code></a>, Inception crop with 5%-100% of the area of the original image (“scale”), random horizontal flips, RandAugment <d-cite key="NEURIPS2020_d85b63ef"></d-cite> with <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandAugment.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">num_ops = 2</code> and <code class="language-plaintext highlighter-rouge">magnitude = 10</code></a>, and Mixup <d-cite key="zhang2018mixup"></d-cite> with <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.MixUp.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">alpha = 0.2</code></a>. We also notice that the Big Vision implementation “normalizes” the image with mean = std = (0.5, 0.5, 0.5) (<a href="https://github.com/google-research/big_vision/blob/46b2456f54b9d4f829d1925b78943372b376153d/big_vision/configs/vit_s16_i1k.py#L52" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">value_range(-1, 1)</code></a>) and uses <a href="https://github.com/google-research/big_vision/blob/46b2456f54b9d4f829d1925b78943372b376153d/big_vision/pp/archive/autoaugment.py#L676" rel="external nofollow noopener noopener noreferrer" target="_blank">the same neutral RGB value as the fill value for RandAugment</a> and make sure that our reproduction conforms to this training setup. Our reproduction attempt reaches 76.91% top-1 ImageNet-1k validation set accuracy vs. 76.7% for the Head: MLP → linear ablation after 90 epochs as given in <d-cite key="beyer2022better"></d-cite>. However, the training loss and gradient L2 norm comparison suggests that they are not equivalent:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/gradient_L2_norm_comparison.png" class="img-fluid" width="auto" height="auto"> </div> <p>It turns out that “RandAugment” in torchVision <d-cite key="torchvision2016"></d-cite> is quite different from “RandAugment” in Big Vision. RandAugment in torchVision, following the description in the paper <d-cite key="NEURIPS2020_d85b63ef"></d-cite>, randomly selects one of the following 14 transforms:</p> <table> <thead> <tr> <th style="text-align: center"> </th> <th style="text-align: center"> </th> <th style="text-align: center"> </th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Identity</td> <td style="text-align: center">AutoContrast</td> <td style="text-align: center">Equalize</td> </tr> <tr> <td style="text-align: center">Rotate</td> <td style="text-align: center">Solarize</td> <td style="text-align: center">Color</td> </tr> <tr> <td style="text-align: center">Posterize</td> <td style="text-align: center">Contrast</td> <td style="text-align: center">Brightness</td> </tr> <tr> <td style="text-align: center">Sharpness</td> <td style="text-align: center">ShearX</td> <td style="text-align: center">ShearY</td> </tr> <tr> <td style="text-align: center">TranslateX</td> <td style="text-align: center">TranslateY</td> <td style="text-align: center"> </td> </tr> </tbody> </table> <p>RandAugment in Big Vision, however, <a href="https://github.com/google-research/big_vision/blob/46b2456f54b9d4f829d1925b78943372b376153d/big_vision/pp/archive/autoaugment.py#L20" rel="external nofollow noopener noopener noreferrer" target="_blank">forked</a> from the EfficientNet reference implementation and has a lineup that consists of 16 transforms:</p> <table> <thead> <tr> <th style="text-align: center"> </th> <th style="text-align: center"> </th> <th style="text-align: center"> </th> </tr> </thead> <tbody> <tr> <td style="text-align: center">AutoContrast</td> <td style="text-align: center">Equalize</td> <td style="text-align: center">Rotate</td> </tr> <tr> <td style="text-align: center">Solarize</td> <td style="text-align: center">Color</td> <td style="text-align: center">Posterize</td> </tr> <tr> <td style="text-align: center">Contrast</td> <td style="text-align: center">Brightness</td> <td style="text-align: center">Sharpness</td> </tr> <tr> <td style="text-align: center">ShearX</td> <td style="text-align: center">ShearY</td> <td style="text-align: center">TranslateX</td> </tr> <tr> <td style="text-align: center">TranslateY</td> <td style="text-align: center">Invert</td> <td style="text-align: center">Cutout</td> </tr> <tr> <td style="text-align: center">SolarizeAdd</td> <td style="text-align: center"> </td> <td style="text-align: center"> </td> </tr> </tbody> </table> <p>with “Identity” no-op removed and “Invert”, “Cutout”, and “SolarizeAdd” added to the list. Moreover, its implementation of the Contrast transform is broken. In short: <a href="https://github.com/google-research/big_vision/blob/01edb81a4716f93a48be43b3a4af14e29cdb3a7f/big_vision/pp/autoaugment.py#L209-L213" rel="external nofollow noopener noopener noreferrer" target="_blank">the <code class="language-plaintext highlighter-rouge">mean</code> here</a></p> <figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="c1"># Compute the grayscale histogram, then compute the mean pixel value,
</span>  <span class="c1"># and create a constant image size of that value.  Use that as the
</span>  <span class="c1"># blending degenerate target of the original image.
</span>  <span class="n">hist</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">histogram_fixed_width</span><span class="p">(</span><span class="n">degenerate</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">],</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
  <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">))</span> <span class="o">/</span> <span class="mf">256.0</span></code></pre></figure> <p>is supposed to be the mean pixel value, but as it is it’s just summing over the histogram (therefore equal to height * width), divided by 256. The correct implementation should be</p> <figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="n">image_height</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">image</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">image_width</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">shape</span><span class="p">(</span><span class="n">image</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
  <span class="c1"># Compute the grayscale histogram, then compute the mean pixel value,
</span>  <span class="c1"># and create a constant image size of that value.  Use that as the
</span>  <span class="c1"># blending degenerate target of the original image.
</span>  <span class="n">hist</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">histogram_fixed_width</span><span class="p">(</span><span class="n">degenerate</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">],</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
  <span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span>
      <span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">hist</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">255.</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span> <span class="o">/</span> <span class="nf">float</span><span class="p">(</span><span class="n">image_height</span> <span class="o">*</span> <span class="n">image_width</span><span class="p">)</span></code></pre></figure> <p>or <a href="https://github.com/tensorflow/models/pull/11219#discussion_r1792502043" rel="external nofollow noopener noopener noreferrer" target="_blank">as suggested by @yeqingli, just call <code class="language-plaintext highlighter-rouge">tf.image.adjust_contrast()</code> for the whole function</a>:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">contrast</span><span class="p">(</span><span class="n">image</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="nf">adjust_contrast</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">factor</span><span class="p">)</span></code></pre></figure> <p>We can experimentally confirm and visualize this using a calibration grid. For example, given the $224 \times 224$ grid below that consists of (192, 64, 64) and (64, 192, 192) squares:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/color_grid.png" class="img-fluid" width="auto" height="auto"> </div> <p><code class="language-plaintext highlighter-rouge">contrast(tf_color_tile, 1.9)</code> before the fix returns a grid with (188, 0, 0) and (0, 188, 188) squares:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/broken_result.png" class="img-fluid" width="auto" height="auto"> </div> <p>After the fix, <code class="language-plaintext highlighter-rouge">contrast(tf_color_tile, 1.9)</code> returns a grid with (249, 6, 6) and (6, 249, 249) squares instead:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/correct_result.png" class="img-fluid" width="auto" height="auto"> </div> <p>Another problematic transform is the Solarize transform, <a href="https://github.com/google-research/big_vision/issues/110" rel="external nofollow noopener noopener noreferrer" target="_blank">whose augmentation strength varies unintuitively with <code class="language-plaintext highlighter-rouge">magnitude</code> and is in danger of uint8 overflow</a>. It just happens to behave as expected with <a href="https://github.com/google-research/big_vision/blob/ec86e4da0f4e9e02574acdead5bd27e282013ff1/big_vision/configs/vit_s16_i1k.py#L57" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">randaug(2, 10)</code></a> in our case.</p> <p>As for the discrepancy between torchVision’s RandAugment and Big Vision’s RandAugment, we can also use a calibration grid to confirm and visualize. Given the following $224 \times 224$ black &amp; white calibration grid:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/calibration_grid.png" class="img-fluid" width="auto" height="auto"> </div> <p>We apply both versions of <code class="language-plaintext highlighter-rouge">RandAugment(2, 10)</code> 100000 times (<a href="https://github.com/EIFY/mup-vit/blob/e35ab281acb88f669b18555603d9187a194ccc2f/notebooks/RandAugmentOnCalibrationGrid.ipynb" rel="external nofollow noopener noopener noreferrer" target="_blank">notebook</a>) to gather the stats. All of the resulting pixels remain colorless (i.e. for RGB values (r, g, b), r == g == b) so we can sort them from black to white into a spectrum (<a href="https://github.com/EIFY/mup-vit/blob/e35ab281acb88f669b18555603d9187a194ccc2f/notebooks/GradientVisual.ipynb" rel="external nofollow noopener noopener noreferrer" target="_blank">notebook</a>). For the following $2000 \times 200$ spectra, pixels are sorted top-down, left-right, and each pixel represents 224 * 224 * 100000 / (2000 * 200) = 112 * 112 pixels of the aggregated output, amounting to 1/4 of one output image. In case one batch of 12544 pixels happens to be of different values, we take the average. Here is the spectrum of torchvision’s <code class="language-plaintext highlighter-rouge">RandAugment(2, 10)</code>:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/torch_vision_randaugment_2_10.png" class="img-fluid" width="auto" height="auto"> </div> <p>Here is the spectrum of torchvision’s <code class="language-plaintext highlighter-rouge">RandAugment(2, 10, fill=[128] * 3)</code>. We can see that it just shifts the zero-padding part of the black to the (128, 128, 128) neutral gray:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/torch_vision_randaugment_2_10_mid_fill.png" class="img-fluid" width="auto" height="auto"> </div> <p>And here is the spectrum of big_vision’s <code class="language-plaintext highlighter-rouge">randaug(2, 10)</code>:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/big_vision_randaugment_2_10.png" class="img-fluid" width="auto" height="auto"> </div> <p>We end up subclassing torchvision’s <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandAugment.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.RandAugment</code></a>(<a href="https://github.com/EIFY/mup-vit/blob/e35ab281acb88f669b18555603d9187a194ccc2f/notebooks/RandAugmentCalibration.ipynb" rel="external nofollow noopener noopener noreferrer" target="_blank">notebook</a>) in order to remove and add transforms to match the lineup of Big Vision’s RandAugment. We use a variety of calibration grids to make sure that all transforms give results within $\pm 1$ of the RGB values given by the Big Vision counterpart, except the Contrast transform for which we decide against replicating the bug. Even with that exception, the near-replication of big_vision’s <code class="language-plaintext highlighter-rouge">randaug(2, 10)</code> results in near-identical spectrum:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/torch_vision_randaugment17_2_10.png" class="img-fluid" width="auto" height="auto"> </div> <p>Before we move on: credit to <a href="https://github.com/tensorflow/tpu/pull/764" rel="external nofollow noopener noopener noreferrer" target="_blank">@tobiasmaier for first noticing the contrast bug and submitting a PR to fix it</a>. Apparently, some CV people are aware of the bug and the discrepancy between the RandAugment implemenations. To our best knowledge however, these issues are not publcally documented anywhere.</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">Yes. This is the original auto/rand_augment code. If you look at images it generates, looks competely broken. That’s why <a href="https://twitter.com/wightmanr?ref_src=twsrc%5Etfw" rel="external nofollow noopener noopener noreferrer" target="_blank">@wightmanr</a> rewrote them more carefully in timm. It means PyTorch randaugment papers mean different things than jax/tf ones.</p>— Lucas Beyer (bl16) (@giffmana) <a href="https://twitter.com/giffmana/status/1798978504689938560?ref_src=twsrc%5Etfw" rel="external nofollow noopener noopener noreferrer" target="_blank">June 7, 2024</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet"> <p lang="en" dir="ltr">I implemented it similar to the OG EfficientNet code at first to reproduce their model training. Noticed several issues, and added flags for alternative behaviour, trained better models :)</p>— Ross Wightman (@wightmanr) <a href="https://twitter.com/wightmanr/status/1799170879697686897?ref_src=twsrc%5Etfw" rel="external nofollow noopener noopener noreferrer" target="_blank">June 7, 2024</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h3 id="inception-crop">Inception crop</h3> <p>Model trained with the near-replication of Big Vision’s <code class="language-plaintext highlighter-rouge">randaug(2, 10)</code> for 90 epochs <a href="https://api.wandb.ai/links/eify/8d0wix47" rel="external nofollow noopener noopener noreferrer" target="_blank">reaches 77.27% top-1 validation set accuracy and the gradient L2 norm looks the same, but the training loss still differs</a>:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/inception_crop_l2_grads_discrepancy.png" class="img-fluid" width="auto" height="auto"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/inception_crop_training_loss_discrepancy.png" class="img-fluid" width="auto" height="auto"> </div> <p>It turns out that besides the default min scale (<a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomResizedCrop.html" rel="external nofollow noopener noopener noreferrer" target="_blank">8%</a> vs. <a href="https://github.com/google-research/big_vision/blob/01edb81a4716f93a48be43b3a4af14e29cdb3a7f/big_vision/pp/ops_image.py#L199" rel="external nofollow noopener noopener noreferrer" target="_blank">5%</a>), the “Inception crop” implemented as torchvision <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomResizedCrop.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.RandomResizedCrop()</code></a> is not the same as calling <a href="https://www.tensorflow.org/api_docs/python/tf/slice" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tf.slice()</code></a> with the bbox returned by <a href="https://www.tensorflow.org/api_docs/python/tf/image/sample_distorted_bounding_box" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box()</code></a> as it is commonly done in JAX / TF:</p> <ol> <li>They both rejection-sample the crop, but <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomResizedCrop.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.RandomResizedCrop()</code></a> is hardcoded to try 10 times while <a href="https://www.tensorflow.org/api_docs/python/tf/image/sample_distorted_bounding_box" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box()</code></a> defaults to 100 attempts.</li> <li> <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomResizedCrop.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.RandomResizedCrop()</code></a> samples the aspect ratio uniformly in log space, <a href="https://www.tensorflow.org/api_docs/python/tf/image/sample_distorted_bounding_box" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box()</code></a> samples uniformly in linear space.</li> <li> <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomResizedCrop.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.RandomResizedCrop()</code></a> samples the area cropped uniformly while <a href="https://www.tensorflow.org/api_docs/python/tf/image/sample_distorted_bounding_box" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box()</code></a> samples the crop height uniformly given the aspect ratio and area range.</li> <li>If all attempts fail, <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomResizedCrop.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.RandomResizedCrop()</code></a> at least crops the image to make sure that the aspect ratio falls within range before resizing. <a href="https://www.tensorflow.org/api_docs/python/tf/image/sample_distorted_bounding_box" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box()</code></a> just returns the whole image to be resized.</li> </ol> <p>We can verify this by taking stats of the crop sizes given the same image. Here is the density plot of (h, w) returned by <code class="language-plaintext highlighter-rouge">v2.RandomResizedCrop.get_params(..., scale=(0.05, 1.0), ratio=(3/4, 4/3))</code>, given an image of (height, width) = (256, 512), N = 10000000:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/torch_hw_counts.png" class="img-fluid" width="auto" height="auto"> </div> <p>There is a total of 14340 crop failures resulting in a bright pixel at the bottom right, but otherwise the density is roughly uniform. In comparison, here is what <code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box(..., area_range=[0.05, 1])</code> returns:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/tf_hw_counts.png" class="img-fluid" width="auto" height="auto"> </div> <p>While cropping never failed, we can see clearly that it’s oversampling smaller crop areas, as if there were light shining from top-left (<a href="https://github.com/EIFY/mup-vit/blob/e35ab281acb88f669b18555603d9187a194ccc2f/notebooks/InceptionCropStats.ipynb" rel="external nofollow noopener noopener noreferrer" target="_blank">notebook</a>). We replicate <code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box()</code>’s sampling logic in PyTorch and rerun the experiment by training a model with it for 90 epochs. At last, both the gradient L2 norm and the training loss match:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/inception_crop_l2_grads_match.png" class="img-fluid" width="auto" height="auto"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/inception_crop_training_loss_match.png" class="img-fluid" width="auto" height="auto"> </div> <p>This true reproduction model reaches 76.94% top-1 validation set accuracy after 90 epochs. In summary:</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">Top-1 val acc.</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Reference</td> <td style="text-align: center">76.7</td> </tr> <tr> <td style="text-align: center">Same ViT init. in PyTorch</td> <td style="text-align: center">76.91</td> </tr> <tr> <td style="text-align: center">+ Same RandAugment</td> <td style="text-align: center"><strong>77.27</strong></td> </tr> <tr> <td style="text-align: center">+ Same Inception Crop</td> <td style="text-align: center">76.94</td> </tr> </tbody> </table> <p>Before we move on: Which implementation is correct? While the reference implementation of the Inception crop is not publicly available to our best knowledge, here is the relevant excerpt from the paper <d-cite key="szegedy2015going"></d-cite>:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/Inception_crop_paper_excerpt.png" class="img-fluid" width="auto" height="auto"> </div> <p>While there is some ambiguity in whether aspect ratio should be sampled uniformly in log space or linear space, it is clear from the description that crop size should be sampled uniformly in crop area.</p> <h3 id="big-vision-miscellaneous">Big Vision miscellaneous</h3> <p>Finally <d-footnote>Except the 2nd and 3rd "grafted" experiments, these experiments were run before the true reproduction chronologically.</d-footnote>, there are a few idiosyncrasies of the Big Vision reference implementation that either are impractical or do not make sense to replicate. Here we run experiments using the Big Vision repo to test their effects on model training.</p> <ol> <li> <p>In <d-cite key="beyer2022better"></d-cite> only the first 99% of the training data is used for training while the remaining 1% is used for minival “to encourage the community to stop selecting design choices on the validation (de-facto test) set”. This however is difficult to replicate with <a href="https://pytorch.org/vision/main/datasets.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torchvision.datasets</code></a> since <a href="https://pytorch.org/vision/main/generated/torchvision.datasets.ImageNet.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">datasets.ImageNet()</code></a> is ordered by class label, unlike <a href="https://www.tensorflow.org/datasets/overview" rel="external nofollow noopener noopener noreferrer" target="_blank">tfds</a> whose ordering is somewhat randomized:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="kn">import</span> <span class="n">tensorflow_datasets</span> <span class="k">as</span> <span class="n">tfds</span>
 <span class="n">ds</span> <span class="o">=</span> <span class="n">tfds</span><span class="p">.</span><span class="nf">builder</span><span class="p">(</span><span class="sh">'</span><span class="s">imagenet2012</span><span class="sh">'</span><span class="p">).</span><span class="nf">as_dataset</span><span class="p">(</span><span class="n">split</span><span class="o">=</span><span class="sh">'</span><span class="s">train[99%:]</span><span class="sh">'</span><span class="p">)</span>
 <span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
 <span class="n">c</span> <span class="o">=</span> <span class="nc">Counter</span><span class="p">(</span><span class="nf">int</span><span class="p">(</span><span class="n">e</span><span class="p">[</span><span class="sh">'</span><span class="s">label</span><span class="sh">'</span><span class="p">])</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">ds</span><span class="p">)</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
 <span class="mi">999</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="nf">max</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
 <span class="mi">27</span>
 <span class="o">&gt;&gt;&gt;</span> <span class="nf">min</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>
 <span class="mi">3</span>
</code></pre></div> </div> <p>Naively trying to do the same with <a href="https://pytorch.org/vision/main/datasets.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torchvision.datasets</code></a> prevents the model from learning the last few classes and results in <a href="https://api.wandb.ai/links/eify/3ju0jben" rel="external nofollow noopener noopener noreferrer" target="_blank">near-random performance on the minival</a>: the model only learns the class that happened to stride across the first 99% and the last 1%. Instead of randomly selecting 99% of the training data or copying the 99% slice given by tfds, we just fall back to training on 100% of the training data.</p> </li> <li> <p>The reference implementation sets <a href="https://github.com/google-research/big_vision/blob/46b2456f54b9d4f829d1925b78943372b376153d/big_vision/configs/vit_s16_i1k.py#L49" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">shuffle_buffer_size = 250_000</code></a> which is only 20% of the training set, so the training data is not fully shuffled. To fit the data in a TensorBook, we need to further reduce it to <code class="language-plaintext highlighter-rouge">150_000</code> and <a href="https://github.com/EIFY/big_vision/commit/e2f74c170d926ab73846ceb9b0d9aad2aa5814af" rel="external nofollow noopener noopener noreferrer" target="_blank">revive the <code class="language-plaintext highlighter-rouge">utils.accumulate_gradient()</code></a> function to train with gradient accumulation. 1 and 2 result in 76.74% top-1 validation set accuracy <d-footnote>Perhaps it's worth noting that training on multiple devices vs. single device is not fully equivalent due to the Mixup implementation. With Mixup, the model is trained with <d-cite key="zhang2018mixup"></d-cite> $$ \begin{align*} \tilde{x} &amp;= \lambda x_i + (1 - \lambda) x_j\\ \tilde{y} &amp;= \lambda y_i + (1 - \lambda) y_j \end{align*} $$ where $(x_i, y_i)$ and $(x_j, y_j)$ are (feature vector, one-hot label) sampled from training data and $\lambda$ is randomly sampled from $[0,1]$. In common implementations including that of Big Vision and torchvision, $\lambda$ is only sampled once per batch and $(x_i, y_i)$ and $(x_j, y_j)$ are always from the same batch. Since each device does such <a href="https://github.com/google-research/big_vision/blob/46b2456f54b9d4f829d1925b78943372b376153d/big_vision/train.py#L290-L291" rel="external nofollow noopener noopener noreferrer" target="_blank">sampling independently</a> in Big Vision, each global batch contains samples mixed-up with $n$ different $\lambda$ values when the model is trained with $n$ devices. That said, we don't expect this to make a difference. Furthermore since both TPUv3-8 and 8x A100-SXM4-40GB have 8 devices total, the Mixup implementation becomes equivalent again after (7.)</d-footnote>.</p> </li> <li> <p>We fix the Contrast transform bug <a href="#randaugment">described above</a>.</p> </li> <li> <p><a href="https://x.com/giffmana/status/1808394534424138049" rel="external nofollow noopener noopener noreferrer" target="_blank">Inadvertently the validation set is anti-aliased</a> because <a href="https://www.tensorflow.org/api_docs/python/tf/image/resize" rel="external nofollow noopener noopener noreferrer" target="_blank">the <code class="language-plaintext highlighter-rouge">area</code> resize method always anti-aliases</a>, but not the training set. We change the resize method to <code class="language-plaintext highlighter-rouge">bilinear</code> with anti-aliasing for both like the default for <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.Resize.html#torchvision.transforms.v2.Resize" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.Reize()</code></a> and <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomResizedCrop.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.RandomResizedCrop()</code></a> in torchvision.</p> </li> <li> <p><a href="https://www.tensorflow.org/api_docs/python/tf/io/decode_jpeg" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tf.io.decode_jpeg()</code> by default lets the system decide the JPEG decompression algorithm</a>. Specifying <code class="language-plaintext highlighter-rouge">dct_method="INTEGER_ACCURATE"</code> makes it <a href="https://github.com/google-research/big_vision/blob/01edb81a4716f93a48be43b3a4af14e29cdb3a7f/big_vision/pp/ops_image.py#L39" rel="external nofollow noopener noopener noreferrer" target="_blank">behave like the PIL/cv2/PyTorch counterpart</a> (see also some quick test at the end of the <a href="https://github.com/EIFY/mup-vit/blob/e35ab281acb88f669b18555603d9187a194ccc2f/notebooks/RandAugmentCalibration.ipynb" rel="external nofollow noopener noopener noreferrer" target="_blank">notebook</a>). <a href="https://github.com/EIFY/big_vision/commit/a31822116a9377d6f6dbfbd78372964ed48d8b9a" rel="external nofollow noopener noopener noreferrer" target="_blank">We expose this option</a> and test it in our experiment. 1-5 result in 76.87% top-1 validation set accuracy.</p> </li> <li> <p><a href="https://optax.readthedocs.io/en/latest/api/transformations.html#optax.scale_by_adam" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">optax.scale_by_adam()</code> supports the unusual option of using a different dtype for the 1st order accumulator, <code class="language-plaintext highlighter-rouge">mu_dtype</code></a> and the reference implementation set it to <a href="https://github.com/google-research/big_vision/blob/46b2456f54b9d4f829d1925b78943372b376153d/big_vision/configs/vit_s16_i1k.py#L80" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">bfloat16</code></a> instead of <code class="language-plaintext highlighter-rouge">float32</code> like the rest of the model. Changing it back to <code class="language-plaintext highlighter-rouge">float32</code> in addition to 1-5 results in <a href="https://api.wandb.ai/links/eify/dr9b8q4w" rel="external nofollow noopener noopener noreferrer" target="_blank">76.77% top-1 validation set accuracy</a>.</p> </li> <li> <p>Lastly, we test whether <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#fully_shuffling_all_the_data" rel="external nofollow noopener noopener noreferrer" target="_blank">fully shuffling the training set</a> helps model performance. We set <a href="https://github.com/EIFY/big_vision/blob/5adab5c5985f0cd9b2e5fd887a58c062866ab092/big_vision/configs/vit_s16_i1k_8_gpu.py#L50" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">config.input.shuffle_buffer_size = 1281167</code></a> and train a model on a 8x A100-SXM4-40GB <a href="https://lambdalabs.com/" rel="external nofollow noopener noopener noreferrer" target="_blank">Lambda</a> instance with 1-6 but no gradient accumulation. The model reaches <a href="https://api.wandb.ai/links/eify/huigfbka" rel="external nofollow noopener noopener noreferrer" target="_blank">76.85% top-1 validation set accuracy</a>.</p> </li> <li> <p>There is one discrepancy that we choose to ignore: In the Big Vision implementation of the Inception crop <a href="https://github.com/google-research/big_vision/blob/46b2456f54b9d4f829d1925b78943372b376153d/big_vision/pp/ops_image.py#L200" rel="external nofollow noopener noopener noreferrer" target="_blank">max (aspect) ratio is set to 1.33</a>, but in <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomResizedCrop.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.RandomResizedCrop()</code> of torchvision</a> it’s set to 4.0/3.0, with difference well above floating point precision. We don’t find it relevant however, and it should be the latter as originally described.</p> </li> </ol> <p>Additionally, for debugging purpose we run 3 “grafted” experiments in which we train and evaluate PyTorch models on the Big Vision data pipelines <d-footnote>The first two grafted experiments are affected by two bugs: One that handles padded input incorrectly and causes the code to underreport top-1 validation set accuracy by a factor of $$\frac{50000}{50176}$$ (<a href="https://github.com/EIFY/big_vision/commit/c6983dbbffaeb21d8c9d7f90caea317b04414fea" rel="external nofollow noopener noopener noreferrer" target="_blank">bugfix commit</a>) and another that flips height and width of the image (<a href="https://github.com/EIFY/big_vision/commit/3159c33be81b99dff19bcbd6d24f527f255ec7df" rel="external nofollow noopener noopener noreferrer" target="_blank">bugfix commit</a>). While the latter doesn't result in equivalent models due to the sincos2d position embeddings, we don't believe that it changes the distribution of the reported metric.</d-footnote>. Other than 1-2, the reference Big Vision data pipelines are not further modified beyond necessity. These 3 grafted experiments reach <a href="https://api.wandb.ai/links/eify/ey4cmaxx" rel="external nofollow noopener noopener noreferrer" target="_blank">76.65-76.81% top-1 validation set accuracy</a>.</p> <p>In summary:</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">Top-1 val acc.</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Reference</td> <td style="text-align: center">76.7</td> </tr> <tr> <td style="text-align: center">100% training set, smaller shuffle buffer, grad. acc.</td> <td style="text-align: center">76.74</td> </tr> <tr> <td style="text-align: center">+ contrast() fix, consistent anti-aliasing, accurate JPEG decode</td> <td style="text-align: center"><strong>76.87</strong></td> </tr> <tr> <td style="text-align: center">+ fp32 1st order acc.</td> <td style="text-align: center">76.77</td> </tr> <tr> <td style="text-align: center">+ full shuffle, no grad. acc.</td> <td style="text-align: center">76.85</td> </tr> <tr> <td style="text-align: center">Grafted # 1</td> <td style="text-align: center">76.65</td> </tr> <tr> <td style="text-align: center">Grafted # 2</td> <td style="text-align: center">76.81</td> </tr> <tr> <td style="text-align: center">Grafted # 3</td> <td style="text-align: center">76.72</td> </tr> </tbody> </table> <p>The metrics suggest that none of them has any effect. However, this doesn’t mean that data pipelines are interchangeable for trained models. Here is what we observe when we evaluate the models trained on the Big Vision data pipeline with the torchvision data pipeline, and vice versa:</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">torchvision pp</th> <th style="text-align: center">Big Vision pp</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Grafted # 2</td> <td style="text-align: center">76.52 <r>-0.29</r> </td> <td style="text-align: center">76.81</td> </tr> <tr> <td style="text-align: center">Grafted # 3</td> <td style="text-align: center">76.47 <r>-0.25</r> </td> <td style="text-align: center">76.72</td> </tr> <tr> <td style="text-align: center">+ Same RandAugment</td> <td style="text-align: center">77.27</td> <td style="text-align: center">77.30 <g>+0.03</g> </td> </tr> <tr> <td style="text-align: center">+ Same Inception Crop</td> <td style="text-align: center">76.94</td> <td style="text-align: center">76.96 <g>+0.02</g> </td> </tr> <tr> <td style="text-align: center">Head: MLP → linear 90ep</td> <td style="text-align: center">76.83</td> <td style="text-align: center">76.86 <g>+0.03</g> </td> </tr> <tr> <td style="text-align: center">Head: MLP → linear 150ep</td> <td style="text-align: center">78.05</td> <td style="text-align: center">77.94 <r>-0.11</r> </td> </tr> <tr> <td style="text-align: center">Head: MLP → linear 300ep</td> <td style="text-align: center">78.87</td> <td style="text-align: center">78.76 <r>-0.11</r> </td> </tr> </tbody> </table> <p>Where the Head: MLP → linear experiments are <code class="language-plaintext highlighter-rouge">+ Same RandAugment</code> models trained in parallel from the next section. For unknown reasons, models trained on the torchvision data pipeline seem more robust. To assess and attribute this surprise discrepancy, we compare the validation set images returned by these two pipelines and measure the L2 difference per pixel after <code class="language-plaintext highlighter-rouge">value_range(-1, 1)</code> rescaling. We find that the tfds pipeline returns nearly all exact images if we specify the decoder to be <code class="language-plaintext highlighter-rouge">tf.io.decode_jpeg(..., dct_method="INTEGER_ACCURATE")</code> <d-footnote>There is only one nonzero L2 value less than 0.01, for some reason.</d-footnote>. The difference emerges with the default decoder <code class="language-plaintext highlighter-rouge">tf.io.decode_image</code> (“Default Decoder”), which is further exacerbated by resizing and cropping (“Preprocessed”, <a href="https://github.com/EIFY/mup-vit/blob/0c2d0ddc23614b0c0990ec15c46a90ed6ee7b444/notebooks/dataset_comparison.ipynb" rel="external nofollow noopener noopener noreferrer" target="_blank">notebook</a>):</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/imagenet-ds-diff.png" class="img-fluid" width="auto" height="auto"> </div> <p>Tentatively, we attribute the discrepancy to the long tail of L2 difference after preprocessing.</p> <h2 id="corrected-reproduction">Corrected reproduction</h2> <p>To further assess the effect of different implementations, we reproduce the full ablation results with 90, 150, and 300 epochs of training budget from <d-cite key="beyer2022better"></d-cite> on a 8x A100-SXM4-40GB instance but with the torchvision Inception crop, <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomResizedCrop.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">v2.RandomResizedCrop()</code></a>. Here is the relevant part of Table 1 from <d-cite key="beyer2022better"></d-cite>:</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">90ep</th> <th style="text-align: center">150ep</th> <th style="text-align: center">300ep</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Our improvements</td> <td style="text-align: center">76.5</td> <td style="text-align: center">78.5</td> <td style="text-align: center">80.0</td> </tr> <tr> <td style="text-align: center">no RandAug+MixUp</td> <td style="text-align: center">73.6</td> <td style="text-align: center">73.7</td> <td style="text-align: center">73.7</td> </tr> <tr> <td style="text-align: center">Posemb: sincos2d → learned</td> <td style="text-align: center">75.0</td> <td style="text-align: center">78.0</td> <td style="text-align: center">79.6</td> </tr> <tr> <td style="text-align: center">Batch-size: 1024 → 4096</td> <td style="text-align: center">74.7</td> <td style="text-align: center">77.3</td> <td style="text-align: center">78.6</td> </tr> <tr> <td style="text-align: center">Global Avgpool → [cls] token</td> <td style="text-align: center">75.0</td> <td style="text-align: center">76.9</td> <td style="text-align: center">78.2</td> </tr> <tr> <td style="text-align: center">Head: MLP → linear</td> <td style="text-align: center">76.7</td> <td style="text-align: center">78.6</td> <td style="text-align: center">79.8</td> </tr> </tbody> </table> <p>Here are our results training the same models with the correct Inception crop. We call their main approach “better baseline” and we round to the same figure to avoid making them seem more accurate than they are:</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">90ep</th> <th style="text-align: center">150ep</th> <th style="text-align: center">300ep</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Better baseline</td> <td style="text-align: center">76.8 <g>+0.3</g> </td> <td style="text-align: center">78.8 <g>+0.3</g> </td> <td style="text-align: center">79.6 <r>-0.4</r> </td> </tr> <tr> <td style="text-align: center">no RandAug+MixUp</td> <td style="text-align: center">73.4 <r>-0.2</r> </td> <td style="text-align: center">73.5 <r>-0.2</r> </td> <td style="text-align: center">73.8 <g>+0.1</g> </td> </tr> <tr> <td style="text-align: center">Posemb: sincos2d → learned</td> <td style="text-align: center">76.2 <g>+1.2</g> </td> <td style="text-align: center">77.7 <r>-0.3</r> </td> <td style="text-align: center">79.6 <g>+0.0</g> </td> </tr> <tr> <td style="text-align: center">Batch-size: 1024 → 4096</td> <td style="text-align: center">75.6 <g>+0.9</g> </td> <td style="text-align: center">77.8 <g>+0.5</g> </td> <td style="text-align: center">78.3 <r>-0.3</r> </td> </tr> <tr> <td style="text-align: center">Global Avgpool → [cls] token</td> <td style="text-align: center">75.3 <g>+0.3</g> </td> <td style="text-align: center">77.1 <g>+0.2</g> </td> <td style="text-align: center">77.5 <r>-0.5</r> </td> </tr> <tr> <td style="text-align: center">Head: MLP → linear</td> <td style="text-align: center">76.8 <g>+0.1</g> </td> <td style="text-align: center">78.1 <r>-0.5</r> </td> <td style="text-align: center">78.9 <r>-0.9</r> </td> </tr> </tbody> </table> <p>We first notice that there is quite a bit of variation: In particular, 76.8(3)% for 90ep Head: MLP → linear is low compared to our previous result 77.27%. Compared to our <a href="https://github.com/EIFY/big_vision/tree/grafted" rel="external nofollow noopener noopener noreferrer" target="_blank">grafted experiments</a> where the variation is $\pm 0.08$ (76.65-76.81%) with random initialization but deterministic (fixed random seed) training data pipeline, we observe variation of $\pm 0.2$ when both initialization and training data pipeline are randomized. Regardless, the trend seems to be that the correct Inception crop benefits lower training budget experiments but shows mixed results for higher training budget. One plausible explanation is that oversampling smaller crops amounts to stronger augmentation, which tends to benefit models in the long run <d-footnote>One might be tempted to try curriculum learning with increasing augmentation strength, but early experiments (<a href="https://github.com/EIFY/mup-vit/tree/curriculum" rel="external nofollow noopener noopener noreferrer" target="_blank">source</a>) result in <a href="https://api.wandb.ai/links/eify/ylnhm3or" rel="external nofollow noopener noopener noreferrer" target="_blank">76.65-77.35 for 90ep Head: MLP → linear</a>, almost the same range.</d-footnote>. We can also see this by <a href="https://api.wandb.ai/links/eify/3joh568g" rel="external nofollow noopener noopener noreferrer" target="_blank">comparing better baseline with no RandAug+MixUp</a>:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/augmentation_tortoise_and_hare.png" class="img-fluid" width="auto" height="auto"> </div> <p>As a setup that is already under-augmented, no RandAug+MixUp itself does not benefit from the correct Inception crop even with 90ep training budget. We also suspect that it may be possible to compensate the Inception crop implementation discrepancy by adjusting augmentation strength otherwise, either through parameters of Inception crop (scale and ratio) or other parts of the augmentation pipeline (RandAugment and Mixup).</p> <h2 id="schedule-free-baseline">Schedule-Free baseline</h2> <p>Since the baselines above use cosine learning rate decay with fixed training budget, the learning rate decays to zero at the end and we can’t continue training the model beyond the budget, a limitation that is subject to much investigation <d-cite key="hagele2024scaling"></d-cite>. More recently, Defazio et al.<d-cite key="defazio2024road"></d-cite> proposed Schedule-Free optimizers which promise to completely sidestep the limitation of fixed training budget by making the training process invariant to it. More specifically, the Schedule-Free AdamW optimizer 1.3 updates the model parameters according to the algorithm:</p> <div class="caption"> <img src="/assets/img/2025-04-28-vit-baseline-revisited/schedule_free_adamw.png" class="img-fluid" width="auto" height="auto"> </div> <div class="caption"> Pseudocode of the Schedule-Free AdamW optimizer 1.3, modified from <b>Algorithm 1</b> in <d-cite key="defazio2024road"></d-cite>. </div> <p>While the gradient is calculated on the \(y\) sequence, the model should be evaluted on the \(x\) sequence, reminiscent of the Polyak-Ruppert average <d-cite key="polyak1992acceleration"></d-cite>. Version 1.3 applies Adam bias-correction to the 2nd moment gradient estimate instead of the LR <a href="https://github.com/facebookresearch/schedule_free/pull/49" rel="external nofollow noopener noopener noreferrer" target="_blank">for stability improvement</a>, with the change highlighted in red above. Our baselines consist of 3 different training budgets, which serve as a great test ground for Schedule-Free optimizers. Furthermore, Schedule-Free AdamW <a href="https://mlcommons.org/2024/08/mlc-algoperf-benchmark-competition/" rel="external nofollow noopener noopener noreferrer" target="_blank">has just won the self-tuning category of the AlgoPerf competition</a> which includes the exact same <code class="language-plaintext highlighter-rouge">imagenet_vit</code> workload that trains ViT-S/16 on ImageNet-1k. It is therefore of great interest to see whether Schedule-Free AdamW can match the metrics of the 3 baselines by just training one model. In our first attempt, we again focus on the “better baseline” main approach and keep all the applicable hyperparameters<d-footnote>torchvision Inception crop, 10000 warm-up steps, 1e-3 maximum learning rate, 1e-4 decoupled weight decay, and gradient clip with <a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html" rel="external nofollow noopener noopener noreferrer" target="_blank">max_norm = 1.0</a></d-footnote> and the Schedule-Free AdamW default. For fair comparison, we make sure that the model is evaluated at the exact same number of steps as the previous 90ep, 150ep, and 300ep experiments. Lastly, for consistency of presentation, we still compare to the metrics reported in <d-cite key="beyer2022better"></d-cite>. Here is the result:</p> <table> <thead> <tr> <th style="text-align: center">Model</th> <th style="text-align: center">90ep</th> <th style="text-align: center">150ep</th> <th style="text-align: center">300ep</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Schedule-Free</td> <td style="text-align: center">76.4 <r>-0.1</r> </td> <td style="text-align: center">78.4 <r>-0.1</r> </td> <td style="text-align: center">79.6 <r>-0.4</r> </td> </tr> </tbody> </table> <p>Extensive subsequent hyperparameter sweeps fail to meaningfully improve upon it. Here is the rest of the first LR sweep:</p> <table align="center"> <tr> <th>LR</th> <th>90ep</th> <th>150ep</th> <th>300ep</th> </tr> <tr> <th>1e-3</th> <th>76.4 <r>-0.1</r> </th> <th>78.4 <r>-0.1</r> </th> <th>79.6 <r>-0.4</r> </th> </tr> <tr> <th>3e-3</th> <th>76.4 <r>-0.1</r> </th> <th>78.2 <r>-0.3</r> </th> <th>79.3 <r>-0.7</r> </th> </tr> <tr> <th>9e-3</th> <td colspan="3" style="text-align:center;"><r>Diverged</r></td> </tr> </table> <p>We then set <a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">max_norm = 100.0</code></a><d-footnote>This effectively removes gradient clipping since it's orders of magnitude larger than the highest logged gradient norm.</d-footnote> and sweep both LR and weight decay (WD):</p> <table align="center"> <tr> <th>LR</th> <th>WD</th> <th>90ep</th> <th>150ep</th> <th>300ep</th> </tr> <tr> <td rowspan="3">1e-3</td> <th>1e-4</th> <th>76.2 <r>-0.3</r> </th> <th>78.2 <r>-0.3</r> </th> <th>79.4 <r>-0.6</r> </th> </tr> <tr> <th>3e-4</th> <th>76.0 <r>-0.5</r> </th> <th>77.7 <r>-0.8</r> </th> <th>79.0 <r>-1.0</r> </th> </tr> <tr> <th>5e-4</th> <th>74.5 <r>-2.0</r> </th> <th>76.0 <r>-2.5</r> </th> <th>77.0 <r>-3.0</r> </th> </tr> <tr> <td rowspan="3">3e-3</td> <th>1e-4</th> <th>76.5 <g>+0.0</g> </th> <th>78.2 <r>-0.3</r> </th> <th>79.4 <r>-0.6</r> </th> </tr> <tr> <th>3e-4</th> <th>76.6 <g>+0.1</g> </th> <th>78.2 <r>-0.3</r> </th> <th>79.1 <r>-0.9</r> </th> </tr> <tr> <th>5e-4</th> <th>75.7 <r>-0.8</r> </th> <th>76.8 <r>-1.7</r> </th> <th>77.5 <r>-2.5</r> </th> </tr> <tr> <td rowspan="3">5e-3</td> <th>1e-4</th> <th>75.4 <r>-1.1</r> </th> <th>77.6 <r>-0.9</r> </th> <th>78.9 <r>-1.1</r> </th> </tr> <tr> <th>3e-4</th> <th>76.6 <g>+0.1</g> </th> <th>78.2 <r>-0.3</r> </th> <th>78.9 <r>-1.1</r> </th> </tr> <tr> <th>5e-4</th> <th>75.9 <r>-0.6</r> </th> <th>76.9 <r>-1.6</r> </th> <th>77.5 <r>-2.5</r> </th> </tr> <tr> <th>9e-3</th> <td colspan="4" style="text-align:center;"><r>Diverged</r></td> </tr> </table> <p>Interestingly, the optimal decoupled weight decay depends on the learning rate, contrary to <d-cite key="adamw-decoupling-blog"></d-cite>. Based on this finding, we just use the hyperparameters of Defazio et al.’s AlgoPerf submission <d-cite key="defazio2024road"></d-cite> and switch back to their coupled weight decay WD=0.08121616522670176, <a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">max_norm = 100.0</code></a>, 2799 warm-up steps, $\beta_2$=0.9955159689799007, and Polynomial in $c_t$ average = 0.75. We then also halve and double the learning rate:</p> <table align="center"> <tr> <th>LR</th> <th>90ep</th> <th>150ep</th> <th>300ep</th> </tr> <tr> <th>1.25e-3</th> <th>76.5 <g>+0.0</g> </th> <th>78.3 <r>-0.2</r> </th> <th>79.4 <r>-0.6</r> </th> </tr> <tr> <th>2.5e-3</th> <th>76.5 <g>+0.0</g> </th> <th>78.1 <r>-0.4</r> </th> <th>79.3 <r>-0.7</r> </th> </tr> <tr> <th>5e-3</th> <td colspan="3" style="text-align:center;"><r>Diverged</r></td> </tr> </table> <p>These 3 sweeps strongly suggest that we are at the limit of the the Schedule-Free AdamW optimizer. It turns out that the <code class="language-plaintext highlighter-rouge">imagenet_vit</code> workload of AlgoPerf <d-cite key="Dahl2023AlgoPerf"></d-cite> sets the validation target <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/3c61cc458015ad004f58d4f70e3100124bcf7df2/algorithmic_efficiency/workloads/imagenet_vit/workload.py" rel="external nofollow noopener noopener noreferrer" target="_blank">at only 77.3% top-1 accuracy and <code class="language-plaintext highlighter-rouge">step_hint</code> at 186_666</a>, meaning that their baseline optimizer sets the learning rate schedule for a fixed budget of 186_666 * 1024 / 1281167 = 149.2, almost 150 epochs. We find that Schedule-Free AdamW indeed reaches 77.3% top-1 validation set accuracy earlier than our 150ep baseline experiment, and the top-1 accuracy curve qualitatively matches Figure 7 in Defazio et al. <d-cite key="defazio2024road"></d-cite>. Given the consistency, we have to conclude that Schedule-Free AdamW can get close but not quite match that of our fixed-budget baselines. Engineering, however, is about tradeoffs. If the last fractions of percentage points of accuracy are vital, it’s still viable to just train a few $10^3\text{–}10^4$ steps longer in exchange of the potential of continuing training indefinitely.</p> <div class="caption"> <a href="https://api.wandb.ai/links/eify/7jdp1wzy" rel="external nofollow noopener noopener noreferrer" target="_blank"><img src="/assets/img/2025-04-28-vit-baseline-revisited/schedule_free_vs_baselines.png" class="img-fluid" width="auto" height="auto"></a> </div> <div class="caption"> Top-1 validation set accuracy, Schedule-Free AdamW vs. fixed-budget baselines. </div> <h2 id="implication">Implication</h2> <p>In the broad sense, the discrepancies in parameter initialization we highlight are likely relevant to most of the models written in PyTorch and JAX, as long as they use the default</p> <ul> <li> <a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.MultiheadAttention</code></a> / <a href="https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.MultiHeadDotProductAttention.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">flax.linen.MultiHeadDotProductAttention</code></a> </li> <li> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.Linear</code></a> / <a href="https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.Dense.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">flax.linen.Dense</code></a> </li> <li> <a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.Conv2d</code></a> and other <code class="language-plaintext highlighter-rouge">torch.nn._ConvNd</code> subclasses / <a href="https://flax.readthedocs.io/en/v0.5.3/_autosummary/flax.linen.Conv.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">flax.linen.Conv</code></a>, or</li> <li> <a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.trunc_normal_" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.init.trunc_normal_()</code></a> / <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.initializers.variance_scaling.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">jax.nn.initializers.variance_scaling</code> with <code class="language-plaintext highlighter-rouge">distribution="truncated_normal"</code></a>.</li> </ul> <p>Even if the user notices the discrepancy, it takes attention to detail to customize correctly. Take the <code class="language-plaintext highlighter-rouge">imagenet_vit</code> workload of AlgoPerf <d-cite key="Dahl2023AlgoPerf"></d-cite>, for example. The <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/86d2a0d23c9a3192f878406edc72547fcf0568ec/algorithmic_efficiency/workloads/imagenet_vit/imagenet_pytorch/models.py" rel="external nofollow noopener noopener noreferrer" target="_blank">PyTorch implementation</a> makes sure that query, key, and value projection matrices are separate and always re-initializes <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">torch.nn.Linear</code></a> layers with <a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">xavier_uniform_()</code></a>. It zero-inits the classification head by default and attempts to <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/86d2a0d23c9a3192f878406edc72547fcf0568ec/algorithmic_efficiency/init_utils.py#L15-L16" rel="external nofollow noopener noopener noreferrer" target="_blank">take the effect of truncation on standard deviation into account</a> for weight initialization. <a href="https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.trunc_normal_" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">trunc_normal_()</code> of PyTorch</a>, however, defaults to truncation at $\pm 2$ instead of $\pm 2$ standard deviation, so the resulting weight ends up almost untruncated:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="n">algorithmic_efficiency.workloads.imagenet_vit.imagenet_pytorch.models</span> <span class="kn">import</span> <span class="n">ViT</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="n">algorithmic_efficiency.workloads.imagenet_vit.workload</span> <span class="kn">import</span> <span class="n">decode_variant</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vit</span> <span class="o">=</span> <span class="nc">ViT</span><span class="p">(</span><span class="o">**</span><span class="nf">decode_variant</span><span class="p">(</span><span class="sh">'</span><span class="s">S/16</span><span class="sh">'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="p">[</span><span class="n">vit</span><span class="p">.</span><span class="n">conv_patch_extract</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">vit</span><span class="p">.</span><span class="n">pre_logits</span><span class="p">.</span><span class="n">weight</span><span class="p">]:</span>
<span class="p">...</span>   <span class="nf">print</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">w</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>
<span class="bp">...</span>
<span class="nf">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2119</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MinBackward1</span><span class="o">&gt;</span><span class="p">)</span> <span class="nf">tensor</span><span class="p">(</span><span class="mf">0.1907</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MaxBackward1</span><span class="o">&gt;</span><span class="p">)</span>
<span class="nf">tensor</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2749</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MinBackward1</span><span class="o">&gt;</span><span class="p">)</span> <span class="nf">tensor</span><span class="p">(</span><span class="mf">0.2512</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MaxBackward1</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div> <p>compared to the <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/86d2a0d23c9a3192f878406edc72547fcf0568ec/algorithmic_efficiency/workloads/imagenet_vit/imagenet_jax/models.py" rel="external nofollow noopener noopener noreferrer" target="_blank">JAX implementation</a>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="n">jax.numpy</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="n">jax.random</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="n">algorithmic_efficiency.workloads.imagenet_vit.imagenet_jax.models</span> <span class="kn">import</span> <span class="n">ViT</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="n">algorithmic_efficiency.workloads.imagenet_vit.workload</span> <span class="kn">import</span> <span class="n">decode_variant</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">vit</span> <span class="o">=</span> <span class="nc">ViT</span><span class="p">(</span><span class="o">**</span><span class="nf">decode_variant</span><span class="p">(</span><span class="sh">'</span><span class="s">S/16</span><span class="sh">'</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">numpy</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">jax</span><span class="p">.</span><span class="n">numpy</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">params</span> <span class="o">=</span> <span class="n">vit</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">key</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">x</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="p">[</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">conv_patch_extract</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">kernel</span><span class="sh">'</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">params</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">pre_logits</span><span class="sh">'</span><span class="p">][</span><span class="sh">'</span><span class="s">kernel</span><span class="sh">'</span><span class="p">]]:</span>
<span class="p">...</span>   <span class="nf">print</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">w</span><span class="p">.</span><span class="nf">max</span><span class="p">())</span>
<span class="bp">...</span>
<span class="o">-</span><span class="mf">0.08204417</span> <span class="mf">0.08203908</span>
<span class="o">-</span><span class="mf">0.11602508</span> <span class="mf">0.116011634</span>
</code></pre></div></div> <p>Since they use the same initialization utility, the PyTorch implementation of the <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/86d2a0d23c9a3192f878406edc72547fcf0568ec/algorithmic_efficiency/workloads/fastmri/fastmri_pytorch/models.py#L75" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">fastmri</code></a>, <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/86d2a0d23c9a3192f878406edc72547fcf0568ec/algorithmic_efficiency/workloads/ogbg/ogbg_pytorch/models.py#L97" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">ogbg</code></a>, and <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/86d2a0d23c9a3192f878406edc72547fcf0568ec/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_pytorch/models.py#L210" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">imagenet_resnet</code></a> workloads are also affected.</p> <p>In the narrow sense, we have shown that the discrepancies in RandAugment and Inception crop implementations impact model performance. Some CV people are aware of the RandAugment issues, but again it requires tremendous attention to detail to keep implementations consistent across ecosystems. As an example: the <code class="language-plaintext highlighter-rouge">imagenet_vit</code> workload of AlgoPerf <d-cite key="Dahl2023AlgoPerf"></d-cite> makes sure that the RandAugment implementations have the same transform lineup and use the same neutral gray fill value. However, the Contrast transform bug of the JAX implementation <a href="https://github.com/mlcommons/algorithmic-efficiency/pull/821" rel="external nofollow noopener noopener noreferrer" target="_blank">remains unfixed</a>. The PyTorch counterpart <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/86d2a0d23c9a3192f878406edc72547fcf0568ec/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_pytorch/randaugment.py#L108" rel="external nofollow noopener noopener noreferrer" target="_blank">simply calls</a> torchvision’s <a href="https://pytorch.org/vision/main/generated/torchvision.transforms.functional.adjust_contrast.html" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">F.adjust_contrast()</code></a> for the correct transform, so the RandAugment ends up not exactly the same.</p> <p>In our opinion though, the Inception crop discrepancy is probably the most severe issue. Given its deep root in training deep image models, it has potentially broad implications and remains undocumented to our best knowledge. Tracking down the commit histories, we find that the schism between the implementations goes back 8 years, almost as old as the paper <d-cite key="szegedy2015going"></d-cite> itself:</p> <ul> <li> <a href="https://github.com/tensorflow/tensorflow/commit/f3a77378b4c056e76691c5eba350a022c11e00d4" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">sample_distorted_bounding_box()</code> has remained unchanged</a>.</li> <li>Hardcoded 10 sampling attempts and uniform crop area sampling <a href="https://github.com/pytorch/vision/commit/d9b8d003d282904461d30e60b9e13ed2f74f3bc6" rel="external nofollow noopener noopener noreferrer" target="_blank">have been the same for <code class="language-plaintext highlighter-rouge">RandomSizedCrop()</code></a>.</li> </ul> <p>Below are some of the papers that we know are affected, the context in which the TF implementation of Inception crop is mentioned, and its current Google Scholar citation counts:</p> <table> <thead> <tr> <th style="text-align: center">Paper</th> <th style="text-align: center">Context</th> <th style="text-align: center">Cited By</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">A Simple Framework for Contrastive Learning of Visual Representations <d-cite key="chen2020simple"></d-cite> </td> <td style="text-align: center">See <a href="https://github.com/google-research/simclr/blob/383d4143fd8cf7879ae10f1046a9baeb753ff438/data_util.py#L285" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box()</code> call in the repo</a>.</td> <td style="text-align: center">20235</td> </tr> <tr> <td style="text-align: center">Big Self-Supervised Models are Strong Semi-Supervised Learners <d-cite key="chen2020big"></d-cite> </td> <td style="text-align: center">See <a href="https://github.com/google-research/simclr/blob/383d4143fd8cf7879ae10f1046a9baeb753ff438/tf2/data_util.py#L279" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box()</code> call in the repo</a>.</td> <td style="text-align: center">2498</td> </tr> <tr> <td style="text-align: center">Scaling Vision Transformers <d-cite key="zhai2022scaling"></d-cite> </td> <td style="text-align: center">See <a href="https://github.com/google-research/big_vision/blob/46b2456f54b9d4f829d1925b78943372b376153d/big_vision/configs/proj/scaling_laws/train_vit_g.py#L49" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">inception_crop(224)</code> in the reference config</a> in Big Vision.</td> <td style="text-align: center">1149</td> </tr> <tr> <td style="text-align: center">How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers <d-cite key="steiner2022how"></d-cite> </td> <td style="text-align: center">Footnote on page 1 writes ‘The code for full reproduction of model training is available at <a href="https://github.com/google-research/big_vision" rel="external nofollow noopener noopener noreferrer" target="_blank">https://github.com/google-research/big_vision</a>.’ and Section 3.4 writes ‘The images are pre-processed by Inception-style cropping (36) and random horizontal flipping.’</td> <td style="text-align: center">650</td> </tr> <tr> <td style="text-align: center">Better plain ViT baselines for ImageNet-1k <d-cite key="beyer2022better"></d-cite> </td> <td style="text-align: center">Big Vision repo link is placed under the authors and Section 2 writes ‘All experiments use “inception crop”’.</td> <td style="text-align: center">108</td> </tr> <tr> <td style="text-align: center">FlexiViT: One Model for All Patch Sizes <d-cite key="beyer2023flexivit"></d-cite> </td> <td style="text-align: center"> <code class="language-plaintext highlighter-rouge">inception_crop</code> can be found in config.json downloaded from the link in the <a href="https://github.com/google-research/big_vision/tree/46b2456f54b9d4f829d1925b78943372b376153d/big_vision/configs/proj/flexivit" rel="external nofollow noopener noopener noreferrer" target="_blank">flexivit project README</a> in Big Vision.</td> <td style="text-align: center">82</td> </tr> <tr> <td style="text-align: center">Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution <d-cite key="dehghani2024patch"></d-cite> </td> <td style="text-align: center">Section 3 writes ‘NaViT is implemented in JAX [21] using the FLAX library [22] and built within Scenic [23].’ and Section 3.5 writes ‘Second, we apply inception-style cropping [35] with a fixed minimum area of 50%’. While the source code of NaViT isn’t available, <a href="https://github.com/search?q=repo%3Agoogle-research%2Fscenic%20sample_distorted_bounding_box&amp;type=code" rel="external nofollow noopener noopener noreferrer" target="_blank">Scenic relies on <code class="language-plaintext highlighter-rouge">tf.image.sample_distorted_bounding_box()</code> for Inception crop</a> and <a href="https://github.com/google-research/scenic/blob/0340172a1ffa97a2cdb02adde7ea6d0ea66e539c/scenic/dataset_lib/dataset_utils.py#L735" rel="external nofollow noopener noopener noreferrer" target="_blank">this call</a> might be what it uses.</td> <td style="text-align: center">51</td> </tr> <tr> <td style="text-align: center">Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design <d-cite key="alabdulmohsin2024getting"></d-cite> </td> <td style="text-align: center">It’s stated in Acknowledgments and Disclosure of Funding that ‘We use the big_vision codebase [10, 9] for conducting experiments in this project’ and <code class="language-plaintext highlighter-rouge">inception_crop</code> can be found in the hyperparameter settings in Appendix B-E.</td> <td style="text-align: center">26</td> </tr> <tr> <td style="text-align: center">Benchmarking Neural Network Training Algorithms <d-cite key="Dahl2023AlgoPerf"></d-cite> </td> <td style="text-align: center">JAX implementation of the ImageNet workloads <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/86d2a0d23c9a3192f878406edc72547fcf0568ec/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/input_pipeline.py#L60" rel="external nofollow noopener noopener noreferrer" target="_blank">relies on <code class="language-plaintext highlighter-rouge">tf.image.stateless_sample_distorted_bounding_box()</code></a> while the PyTorch counterpart <a href="https://github.com/mlcommons/algorithmic-efficiency/blob/86d2a0d23c9a3192f878406edc72547fcf0568ec/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_pytorch/workload.py#L99" rel="external nofollow noopener noopener noreferrer" target="_blank">invokes torchvision’s <code class="language-plaintext highlighter-rouge">transforms.RandomResizedCrop()</code></a>.</td> <td style="text-align: center">18</td> </tr> </tbody> </table> <p>Now, how would the models trained for the highly-cited papers above including SimCLR, SoViT-400M, and FlexViT behave differently if they were trained with the correct Inception crop?</p> <h2 id="conclusion">Conclusion</h2> <p>We wish that things just worked — The same models, the same layers, the same initializations, and the same data augmentations mean the same things across ecosystems — but that is not the world we live in. To get closer to that world, all of us need to be more proactive in documenting discrepancies and fixing bugs in existing implementations.</p> <p>And our own repo should be no exception.</p> <h3 id="replication-guide">Replication guide</h3> <p>Selected model checkpoints are available on Hugging Face <a href="https://huggingface.co/EIFY/ViT_Baseline_Revisited" rel="external nofollow noopener noopener noreferrer" target="_blank">here</a>.</p> <ol> <li> <a href="https://pytorch.org/" rel="external nofollow noopener noopener noreferrer" target="_blank">Install PyTorch</a>, including torchvision.</li> <li>Prepare the <a href="https://pytorch.org/vision/main/generated/torchvision.datasets.ImageNet.html" rel="external nofollow noopener noopener noreferrer" target="_blank">ILSVRC 2012 ImageNet-1k dataset</a>.</li> <li><code class="language-plaintext highlighter-rouge">git clone https://github.com/EIFY/mup-vit.git</code></li> <li> <code class="language-plaintext highlighter-rouge">pip install wandb</code> or make changes to use your preferred logging platform.</li> <li> <code class="language-plaintext highlighter-rouge">pip install schedulefree</code> or <code class="language-plaintext highlighter-rouge">git checkout add6d47</code> in the repo if you don’t plan to run Schedule-Free experiments.</li> <li>Depending on your machine: <ul> <li>Multi-node, multi-GPU: The code is intended to support but currently not tested on a multi-node, multi-GPU setup.</li> <li>Single-node, multi-GPU: Say the ImagNet-1k dataset is at <code class="language-plaintext highlighter-rouge">/data/ImageNet/</code>. Just point <code class="language-plaintext highlighter-rouge">MUPVIT_MAIN</code> to <code class="language-plaintext highlighter-rouge">main.py</code> in the repo and run the bash script <a href="https://github.com/EIFY/mup-vit/blob/213a229852d65dbaf494ecb982540c804b607c46/scripts/baseline_revisited.sh" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">scripts/baseline_revisited.sh</code></a>:</li> </ul> <div class="language-bash highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="c">#!/bin/bash</span>

 <span class="nv">MUPVIT_MAIN</span><span class="o">=</span>~/Downloads/mup-vit/main.py
 <span class="nv">PYTHON</span><span class="o">=</span>torchrun
 <span class="nv">N_WORKERS</span><span class="o">=</span>80
 <span class="nv">N_THREADS</span><span class="o">=</span>124

 <span class="k">for </span>EPOCH <span class="k">in </span>90 150 300
 <span class="k">do
     </span><span class="nv">NUMEXPR_MAX_THREADS</span><span class="o">=</span><span class="nv">$N_THREADS</span> <span class="nv">$PYTHON</span> <span class="nv">$MUPVIT_MAIN</span> /data/ImageNet/ <span class="nt">--workers</span> <span class="nv">$N_WORKERS</span> <span class="nt">--multiprocessing-distributed</span> <span class="nt">--epochs</span> <span class="nv">$EPOCH</span> <span class="nt">--batch-size</span> 1024 <span class="nt">--torchvision-inception-crop</span> <span class="nt">--mlp-head</span> <span class="nt">--report-to</span> wandb <span class="nt">--name</span> better-baseline-<span class="k">${</span><span class="nv">EPOCH</span><span class="k">}</span>ep
 <span class="k">done</span>
 <span class="c"># (...)</span>
</code></pre></div> </div> <p>Remove <code class="language-plaintext highlighter-rouge">--torchvision-inception-crop</code> for faithful replication of <d-cite key="beyer2022better"></d-cite>, i.e. with TF-like Inception crop. Experimentally step/s hits a plateau with 64-112 workers on a Lambda 8x A100-SXM4-40GB instance. Adjust both <code class="language-plaintext highlighter-rouge">N_WORKERS</code> and <code class="language-plaintext highlighter-rouge">N_THREADS</code> for your machine.</p> <ul> <li>Single-node, single-GPU: Replace <code class="language-plaintext highlighter-rouge">torchrun</code> with <code class="language-plaintext highlighter-rouge">python</code> and remove <code class="language-plaintext highlighter-rouge">--multiprocessing-distributed</code>. You may need to use gradient accumulation to make sure \(\left( \frac{\text{batch-size}}{\text{accum-freq}} \right)\) samples fit in the GPU RAM:</li> </ul> <div class="language-bash highlighter-rouge"> <div class="highlight"><pre class="highlight"><code> <span class="c">#!/bin/bash</span>

 <span class="nv">MUPVIT_MAIN</span><span class="o">=</span>~/Downloads/mup-vit/main.py
 <span class="nv">PYTHON</span><span class="o">=</span>python
 <span class="nv">N_WORKERS</span><span class="o">=</span>4
 <span class="nv">ACC_FREQ</span><span class="o">=</span>8

 <span class="k">for </span>EPOCH <span class="k">in </span>90 150 300
 <span class="k">do</span>
     <span class="nv">$PYTHON</span> <span class="nv">$MUPVIT_MAIN</span> /data/ImageNet/ <span class="nt">--workers</span> <span class="nv">$N_WORKERS</span> <span class="nt">--epochs</span> <span class="nv">$EPOCH</span> <span class="nt">--batch-size</span> 1024 <span class="nt">--accum-freq</span> <span class="nv">$ACC_FREQ</span> <span class="nt">--torchvision-inception-crop</span> <span class="nt">--mlp-head</span> <span class="nt">--report-to</span> wandb <span class="nt">--name</span> better-baseline-<span class="k">${</span><span class="nv">EPOCH</span><span class="k">}</span>ep
 <span class="k">done</span>
 <span class="c"># (...)</span>
</code></pre></div> </div> <p>Warning: On a single GPU these experiments will likely take a long time.</p> </li> <li>For the Schedule-Free AdamW sweeps, see <a href="https://github.com/EIFY/mup-vit/blob/213a229852d65dbaf494ecb982540c804b607c46/scripts/schedule_free.sh" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">scripts/schedule_free.sh</code></a>, <a href="https://github.com/EIFY/mup-vit/blob/213a229852d65dbaf494ecb982540c804b607c46/scripts/schedule_free_sweep.sh" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">scripts/schedule_free_sweep.sh</code></a>, and <a href="https://github.com/EIFY/mup-vit/blob/213a229852d65dbaf494ecb982540c804b607c46/scripts/schedule_free_algoperf.sh" rel="external nofollow noopener noopener noreferrer" target="_blank"><code class="language-plaintext highlighter-rouge">scripts/schedule_free_algoperf.sh</code></a>.</li> </ol> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/assets/bibliography/2025-07-03-vit-baseline-revisited.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>